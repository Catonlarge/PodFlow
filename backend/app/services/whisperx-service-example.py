import sys
import os
import time  # [æ–°å¢ 1] å¯¼å…¥æ—¶é—´æ¨¡å—
from pathlib import Path


# æ·»åŠ é¡¹ç›®è·¯å¾„ï¼ˆå¿…é¡»åœ¨å¯¼å…¥ app æ¨¡å—ä¹‹å‰ï¼‰
current_file = Path(__file__).resolve()
backend_dir = current_file.parent.parent.parent  # services -> app -> backend
if str(backend_dir) not in sys.path:
    sys.path.insert(0, str(backend_dir))

# ç°åœ¨å¯ä»¥å®‰å…¨å¯¼å…¥ app æ¨¡å—äº†
from app.config import HF_TOKEN


import sys
import os
import time
import gc 
from pathlib import Path

# ==========================================
# [ç»ˆæè¡¥ä¸åŒº] RTX 5070 + PyTorch Nightly å…¨å…¼å®¹ä¿®å¤
# ==========================================
import torch
import torchaudio

# 1. é’ˆå¯¹ VAD/Diarizationï¼šæ·»åŠ  Omegaconf ç™½åå•
try:
    from omegaconf import ListConfig, DictConfig
    torch.serialization.add_safe_globals([ListConfig, DictConfig])
except ImportError:
    pass
except Exception:
    pass

# 2. å¼ºåˆ¶å…³é—­ weights_only æ£€æŸ¥ (è§£å†³ pyannote æ¨¡å‹åŠ è½½æŠ¥é”™)
try:
    _original_torch_load = torch.load
    def safe_load_wrapper(*args, **kwargs):
        kwargs['weights_only'] = False
        return _original_torch_load(*args, **kwargs)
    torch.load = safe_load_wrapper
except Exception:
    pass

# 3. ä¿®å¤ torchaudio Nightly ç¼ºå°‘çš„ API
if not hasattr(torchaudio, "AudioMetaData"):
    try:
        from torchaudio.backend.common import AudioMetaData
        setattr(torchaudio, "AudioMetaData", AudioMetaData)
    except ImportError:
        from dataclasses import dataclass
        @dataclass
        class AudioMetaData:
            sample_rate: int
            num_frames: int
            num_channels: int
            bits_per_sample: int
            encoding: str
        setattr(torchaudio, "AudioMetaData", AudioMetaData)

if not hasattr(torchaudio, "list_audio_backends"):
    def _mock_list_audio_backends():
        return ["soundfile"] 
    setattr(torchaudio, "list_audio_backends", _mock_list_audio_backends)

if not hasattr(torchaudio, "get_audio_backend"):
    def _mock_get_audio_backend():
        return "soundfile"
    setattr(torchaudio, "get_audio_backend", _mock_get_audio_backend)

# ==========================================

import whisperx
from whisperx.diarize import DiarizationPipeline

# æ·»åŠ é¡¹ç›®è·¯å¾„
current_file = Path(__file__).resolve()
backend_dir = current_file.parent.parent.parent
if str(backend_dir) not in sys.path:
    sys.path.insert(0, str(backend_dir))

# ==========================================
# [é…ç½®åŒº] è¯·åœ¨æ­¤å¤„å¡«å…¥ä½ çš„ Token
# ==========================================
# âš ï¸ å¿…é¡»åœ¨ HuggingFace åŒæ„ pyannote/segmentation-3.0 çš„åè®®

def transcribe_demo():
    print("\n--- WhisperX å…¨æµç¨‹ï¼šè½¬å½• + å¯¹é½ + è§’è‰²åŒºåˆ† ---")
    
    # 1. ç¡¬ä»¶å‡†å¤‡
    if torch.cuda.is_available():
        device = "cuda"
        compute_type = "float16"
        print(f"âœ… ç¡¬ä»¶å°±ç»ª: {torch.cuda.get_device_name(0)}")
    else:
        device = "cpu"
        compute_type = "int8"
        print("âš ï¸ ä½¿ç”¨ CPU è¿è¡Œ")

    audio_file = r"D:\programming_enviroment\learning-EnglishPod3\backend\data\audio\003.mp3"
    model_dir = r"D:\programming_enviroment\learning-EnglishPod3\backend\data\transcript"
    
    if not os.path.exists(model_dir):
        os.makedirs(model_dir)

    try:
        if not os.path.exists(audio_file):
            print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ°éŸ³é¢‘æ–‡ä»¶ {audio_file}")
            return

        # ==========================================
        # Step 1: è½¬å½• (Transcribe)
        # ==========================================
        print(f"\nğŸš€ [Step 1/3] æ­£åœ¨åŠ è½½ Whisper æ¨¡å‹å¹¶è½¬å½•...")
        t0 = time.time()
        
        # åŠ è½½è½¬å½•æ¨¡å‹
        model = whisperx.load_model("large-v2", device, compute_type=compute_type, download_root=model_dir)
        
        # åŠ è½½éŸ³é¢‘
        audio = whisperx.load_audio(audio_file)
        
        # æ‰§è¡Œè½¬å½•
        result = model.transcribe(audio, batch_size=16)
        
        # é‡Šæ”¾æ˜¾å­˜ï¼šè½¬å½•æ¨¡å‹ç”¨å®Œå³å¼ƒï¼ˆä¸ºäº†ç»™åé¢çš„æ¨¡å‹è…¾åœ°æ–¹ï¼Œè™½ç„¶ 5070 æ˜¾å­˜å¾ˆå¤§ï¼‰
        # gc.collect()
        # torch.cuda.empty_cache()
        # del model
        
        t1 = time.time()
        print(f"âœ… è½¬å½•å®Œæˆ (è€—æ—¶ {t1 - t0:.2f}s) | è¯†åˆ«è¯­è¨€: {result['language']}")

        # ==========================================
        # Step 2: å¯¹é½ (Align)
        # ==========================================
        print(f"\nğŸš€ [Step 2/3] æ­£åœ¨åŠ è½½å¯¹é½æ¨¡å‹å¹¶æ ¡å‡†æ—¶é—´æˆ³...")
        t2 = time.time()
        
        # åŠ è½½ä¸“é—¨çš„å¯¹é½æ¨¡å‹ï¼ˆWav2Vec2ï¼‰
        model_a, metadata = whisperx.load_align_model(language_code=result["language"], device=device)
        
        # æ‰§è¡Œå¯¹é½
        result = whisperx.align(result["segments"], model_a, metadata, audio, device, return_char_alignments=False)
        
        # é‡Šæ”¾æ˜¾å­˜
        # gc.collect()
        # torch.cuda.empty_cache()
        # del model_a
        
        t3 = time.time()
        print(f"âœ… å¯¹é½å®Œæˆ (è€—æ—¶ {t3 - t2:.2f}s)")

        # ==========================================
        # Step 3: è¯´è¯äººåŒºåˆ† (Diarization)
        # ==========================================
        print(f"\nğŸš€ [Step 3/3] æ­£åœ¨åŠ è½½ Diarization æ¨¡å‹åŒºåˆ†è§’è‰²...")
        t4 = time.time()
        
        # åŠ è½½ Pyannote æ¨¡å‹ (éœ€è¦ HF Token)
        # æ³¨æ„ï¼šè¿™é‡Œä¼šè§¦å‘ weights_only æ£€æŸ¥ï¼Œæˆ‘ä»¬ä¸Šé¢çš„è¡¥ä¸è‡³å…³é‡è¦
        diarize_model = DiarizationPipeline(use_auth_token=HF_TOKEN, device=device)
        
        # æ‰§è¡ŒåŒºåˆ†
        # min_speakers å’Œ max_speakers å¯é€‰ï¼Œå¦‚æœä¸ç¡®å®šå°±å»æ‰
        diarize_segments = diarize_model(audio)
        
        # å°†è§’è‰²æ ‡ç­¾åˆå¹¶å›è½¬å½•ç»“æœ
        result = whisperx.assign_word_speakers(diarize_segments, result)
        
        t5 = time.time()
        print(f"âœ… è§’è‰²åŒºåˆ†å®Œæˆ (è€—æ—¶ {t5 - t4:.2f}s)")
        
        print(f"\nğŸ å…¨æµç¨‹æ€»è€—æ—¶: {t5 - t0:.2f} ç§’")

        # ==========================================
        # ç»“æœå±•ç¤º
        # ==========================================
        print("\n--- æœ€ç»ˆå‰§æœ¬é¢„è§ˆ ---")
        for seg in result["segments"]:
            # æŸäº›ç‰‡æ®µå¯èƒ½æ— æ³•è¯†åˆ«è¯´è¯äººï¼Œå¤„ç† KeyError
            speaker = seg.get("speaker", "Unknown")
            start = seg['start']
            end = seg['end']
            text = seg['text'].strip()
            print(f"[{start:6.2f}s -> {end:6.2f}s] {speaker}: {text}")

    except Exception as e:
        print(f"\nâŒ å‘ç”Ÿé”™è¯¯: {e}")
        if "401 Client Error" in str(e) or "403 Client Error" in str(e):
            print("\nğŸš¨ é‰´æƒå¤±è´¥æç¤ºï¼š")
            print("1. è¯·ç¡®ä¿ä½ åœ¨ä»£ç é¡¶éƒ¨å¡«å…¥äº†æ­£ç¡®çš„ HF_TOKEN")
            print("2. è¯·ç¡®ä¿ä½ å·²åœ¨ HuggingFace å®˜ç½‘æ¥å—äº† 'pyannote/segmentation-3.0' çš„ç”¨æˆ·åè®®")

if __name__ == "__main__":
    transcribe_demo()