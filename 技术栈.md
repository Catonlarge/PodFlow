# 🛠️ PodFlow 技术栈文档 (Tech Stack)

本项目采用 **前后端分离 (Client-Server)** 架构，遵循 **本地优先 (Local-First)** 原则。旨在利用现代 Web 技术构建流畅的用户体验，同时通过本地 AI 模型确保数据隐私与零边际成本。

### 🖥️ 前端 (Frontend)

> **职责：** 负责界面渲染、用户交互、音频播放以及向后端发起数据请求。

* **React (构建于 Vite):**
* **作用：** 项目的骨架。Vite 提供了极速的开发环境启动和打包体验，React 负责构建响应式的组件化界面（如左右分栏、笔记跟随）。


* **MUI (Material UI):**
* **作用：** 项目的“装修”。使用 Google 风格的现成组件库（按钮、滑块、卡片、导航栏），避免陷入手写 CSS 的时间黑洞，确保 V1 版本界面整洁专业。


* **Axios:**
* **作用：** 前后端的“联络员”。比原生 fetch 更智能，自动处理 JSON 数据转换和错误拦截，负责将用户的操作（如“保存笔记”）发送给 Python 后端。



### ⚙️ 后端 (Backend)

> **职责：** 负责业务逻辑处理、AI 模型调用、文件管理及数据库读写。

* **Python (3.8+):**
* **作用：** 核心逻辑语言。因其在 AI 领域的绝对统治力，用于串联所有智能功能。


* **FastAPI:**
* **作用：** 高性能 Web 框架。它像一个交通枢纽，定义了标准的 HTTP 接口（API），让前端可以通过 URL（如 `/transcribe`）来指挥后端干活。自带的 Swagger 文档方便调试。


* **SQLAlchemy:**
* **作用：** 数据库管理工具 (ORM)。充当“翻译官”，让我们能用 Python 代码直接操作数据库对象，而无需编写易错的 SQL 语句，提高了代码的可维护性和安全性。



### 🧠 核心能力 (Core & AI)

> **职责：** 实现产品的核心价值——听写与存储。

* **OpenAI Whisper (Local):**
* **作用：** 离线语音识别模型。直接在用户设备上运行，将 MP3 精准转录为带时间戳的文本。**零 API 成本，且无需上传音频到云端。**


* **FFmpeg:**
* **作用：** 多媒体处理引擎。幕后的脏活累活处理者，负责在转录前将各种复杂的音频格式统一压缩、转换，确保 Whisper 能顺利读取。


* **SQLite:**
* **作用：** 嵌入式数据库。整个应用的所有数据（字幕、笔记、生词）都存储在一个本地的 `.db` 文件中。无需安装服务器，即插即用，方便备份与迁移。



### 🔧 基础设施 (Infrastructure)

> **职责：** 支撑开发环境与远程访问。

* **Node.js (LTS):**
* **作用：** 仅作为构建工具的运行环境。它负责把 React 代码“编译”成浏览器能懂的文件，**不参与**实际的业务逻辑运行。


* **Tailscale:**
* **作用：** 安全隧道。在不暴露公网 IP 的前提下，建立加密的虚拟局域网，实现 iPad/手机在户外对家中 PC 服务的安全访问。



---

### 💡 为什么这么选？(Design Philosophy)

* **简单至上 (Simplicity):** 移除了 Docker、LangChain 等对于 MVP 阶段过于复杂的工具，降低认知负担。
* **成本为零 (Zero Cost):** 通过本地 Whisper + SQLite，避免了昂贵的云数据库和 Token 计费，适合个人长期使用。
* **体验优先 (UX First):** 选择 React + MUI 而非纯 Python GUI，是为了实现更细腻的富文本交互和多端（iPad）适配能力。